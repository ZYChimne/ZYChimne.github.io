[{"content":"Tree Binary Tree 1 2 3 4 5 6 7 8 9 10 11 12 13 /* * * Definition for a binary tree node. * From LeetCode. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) * : val(x), left(left), right(right) {} * }; */ Delete a Node in Binary Tree 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 TreeNode* delete_node(TreeNode* root, int key) { if(root==nullptr) return nullptr; else if(root-\u0026gt;val\u0026gt;key) { root-\u0026gt;left=delete_node(root-\u0026gt;left, key); } else if(root-\u0026gt;val\u0026lt;key) { root-\u0026gt;right=delete_node(root-\u0026gt;right, key); } else { if(root-\u0026gt;left==nullptr) return root-\u0026gt;right; else if(root-\u0026gt;right==nullptr) return root-\u0026gt;left; else { TreeNode* new_root=root-\u0026gt;right; while(new_root-\u0026gt;left!=nullptr) new_root=new_root-\u0026gt;left; new_root-\u0026gt;right=delete_node(root-\u0026gt;right, new_root-\u0026gt;val); new_root-\u0026gt;left=root-\u0026gt;left; return new_root; } } return root; } Segment Tree My Calendar III (Also Differentiation \u0026amp; Prefix Sum)\nSegment Tree OI-WIKI\nSegment Tree is efficient to tell how many times a point has appeared in a segment.\nTrie (Prefix Tree) Implement Trie\nString Split C++ string doesn\u0026rsquo;t have built-in split function. To split string into an array of substrings by a character,\n1 2 3 4 5 6 7 8 9 vector\u0026lt;string\u0026gt; split(string str, char c) { stringstream ss(\u0026amp;str); vector\u0026lt;string\u0026gt;res; string temp; while(getline(ss, temp, c)){ res.emplace_back(temp); } return res; } KMP Heuristic string matching algorithm time complexity is O(m*n). KMP (Knuth Morris Pratt) Pattern Searching algorithm uses the previous matching result to optimize the searching process.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 vector\u0026lt;int\u0026gt;longest_prefix_array; void compute_longest_prefix_array(const string\u0026amp; pattern) { longest_prefix_array.resize(pattern.size(), -1); for(int i=1; i\u0026lt;pattern.size(); i++) { int j=longest_prefix_array[i-1]; while(j!=-1\u0026amp;\u0026amp;pattern[j+1]!=pattern[i]) { j=longest_prefix_array[j]; } if(pattern[j+1]==pattern[i]) { longest_prefix_array[i]=j+1; } } } bool kmp_search(const string\u0026amp; str, const string\u0026amp; pattern) { compute_longest_prefix_array(pattern); for(int i=1, j=-1; i\u0026lt;str.size()-1; i++) { while(j!=-1\u0026amp;\u0026amp;pattern[j+1]!=str[i]) { j=longest_prefix_array[j]; } if(pattern[j+1]==str[i]) { j++; if(j==str.size()-1) return true; } } return false; // mismatch } Sort Insertion Sort Insertion sort outperforms other sorting algorithms in small sizes and is stable.\n1 2 3 4 5 6 7 8 9 10 11 void insertion_sort(vector\u0026lt;int\u0026gt;\u0026amp; arr) { for(int i=1; i\u0026lt;arr.size(); i++) { int temp=arr[i], j=i; // Move greater elements to positions of greater indexes while(arr[j]\u0026gt;temp) { arr[j+1]=arr[j]; j--; } arr[j+1]=temp; } } Quick Sort Quick Sort is preferred over Merge Sort for arrays.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 vector\u0026lt;int\u0026gt;arr; /* This function takes last element as pivot, places the pivot element at its correct position in sorted array, and places all smaller (smaller than pivot) to left of pivot and all greater elements to right of pivot */ int partition(int low, int high) { int pivot=arr[high]; // pivot // Index of smaller element // and indicates the right position of pivot found so far int i=(low-1); for (int j=low; j\u0026lt;=high-1; j++) { // If current element is smaller than the pivot if (arr[j]\u0026lt;pivot) { i++; // increment index of smaller element swap(arr[i], arr[j]); } } swap(arr[i + 1], arr[high]); return (i + 1); } /* The main function that implements QuickSort arr[] --\u0026gt; Array to be sorted, low --\u0026gt; Starting index, high --\u0026gt; Ending index */ void quickSort(int low, int high) { if (low \u0026lt; high) { /* pi is partitioning index, arr[p] is now at right place */ int pi = partition(low, high); // Separately sort elements before // partition and after partition quickSort(low, pi - 1); quickSort(pi + 1, high); } } Merge Sort Merge Sort is preferred over Quick Sort for linked lists. Merge Sort\nHybrid Sort Hybrid sort uses different sorting algorithms in different scenarios, and thus maintaining high performance across the cases. For example, pattern defeating sort use insertion sort when the partition of target is slower than a preset threshold (24). For larger target size, it applies quicksort. Once it learns the partition of target is bad in its definition, it uses a fallback strategy (heap sort). Other hybrid sorts include introsort, timsort, etc.\nHeap Stack Random Mersenne Twister (MT19937) The Mersenne Twister is a general-purpose pseudorandom number generator (PRNG). Its name derives from the fact that its period length is chosen to be a Mersenne Prime. Read More on Wikipedia. MT19937 stands for mersenne twister with a long period of 219937 – 1 which means mt19937 produces a sequence of 32-bit integers that only repeats itself after 2^19937 – 1 number have been generated.\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;ctime\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;random\u0026gt; using namespace std; int main() { // Using the constructor to // initialize with a seed value mt19937 mt(time(nullptr)); // Operator() is used to // generate random numbers cout \u0026lt;\u0026lt; mt(); return 0; } Search Binary Search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // assume target in candidates vector\u0026lt;int\u0026gt;candidates; // return index of target in candidates int binary_search(int target) { int low=0, high=candidates.size(); while(low\u0026lt;high) { int mid=(high-low)/2+low; if(candidates[mid]==target) { return mid; } else if(candidates[mid]\u0026gt;target) { high=mid; } else { low=mid+1; } } return low; } Union Find With Path Compression 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class UnionFind { public: UnionFind(int n) { for(int i=0; i\u0026lt;n; i++) { parent.push_back(i); rank.push_back(0); } } int find(int x) { if(parent[x]!=x) { parent[x]=find(parent[x]); } return parent[x]; } void unite(int x, int y) { x=find(x); y=find(y); if(x==y) return; if(rank[x]\u0026lt;rank[y]) { parent[x]=y; } else { parent[y]=x; if(rank[x]==rank[y]) rank[x]++; } } bool same(int x, int y) { return find(x)==find(y); } private: vector\u0026lt;int\u0026gt;parent; vector\u0026lt;int\u0026gt;rank; }; ","date":"2022-06-10T00:00:00Z","permalink":"https://zychimne.github.io/posts/algorithms/","title":"Algorithms"},{"content":"Bit Manipulation is elegant, as it operates on bits and has unparalleled efficiency. It is also confusing and difficult to understand, because its algorithms are not intuitive. Here is my collection of bit manipulation tricks.\nSimple Bit Manipulation Tricks Set Union: A | B Set Intersection: A \u0026amp; B Set Subtraction: A \u0026amp; ~B Set Negation of All Bits: ^A or ~A Set Bit of A: A |= (1 \u0026laquo; bit) Clear Bit: A \u0026amp; ~(1 \u0026laquo; bit) Test Bit: (A \u0026amp; (1 \u0026laquo; bit)) != 0 Toggle Bit: (A ^ (1 \u0026laquo; bit)) Extract Last Bit: A \u0026amp; ~A or A \u0026amp; (A-1) or A ^ (A \u0026amp; (A-1)) Remove Last Bit: A \u0026amp; (A-1) Get All 1 Bits: ~0 Isolate Rightmost Bit: A \u0026amp; (-A) Propagate Rightmost Bit: (A | (A-1)) Isolate Rightmost 0: ~A \u0026amp; (A+1) Turn on Rightmost 0 Bit: (A | (A+1)) Contrast Number: ~A + 1 or (A ^ -1) + 1 Modulo 2^A against B: A \u0026amp; (B - 1); Absolute Value: (A ^ (A \u0026raquo; 31)) - (A \u0026raquo; 31) Get Max Value: B \u0026amp; ((A-B) \u0026raquo; 31) | A \u0026amp; (~(A-B) \u0026raquo; 31) Get Min Value: A \u0026amp; ((A-B) \u0026raquo; 31) | B \u0026amp; (~(A-B) \u0026raquo; 31) Count the Number of Ones 1 2 3 4 5 6 7 int count_one(int n) { while(n) { n = n\u0026amp;(n-1); count++; } return count; } Is Power of 2 1 2 3 int is_power_of_two(int n) { return !(n\u0026amp;(n-1)); } Is Power of 4 1 2 3 4 int mask = 0x55555555 // check the 1-bit location; bool is_power_of_four(int n) { return !(n\u0026amp;(n-1)) \u0026amp;\u0026amp; (n\u0026amp;mask); } Sum of 2 Integers 1 2 3 4 int sum(int a, int b) { // be careful about the terminating condition return b==0? a:sum(a^b, (a\u0026amp;b)\u0026lt;\u0026lt;1); } 1 2 3 4 5 6 7 8 int sum(int a, int b) { while(b!=0){ unsigned int c=(unsigned int)(a\u0026amp;b)\u0026lt;\u0026lt;1; a=a^b; b=c; } return a; } Missing Number Given an array containing n distinct numbers taken from 0, 1, 2, \u0026hellip;, n, find the one that is missing from the array.\n1 2 3 4 5 6 7 8 int missing_number(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int ret = 0; for(int i = 0; i \u0026lt; nums.size(); ++i) { ret ^= i; ret ^= nums[i]; } return ret^=nums.size(); } The Most Significant Bit Find the largest power of 2 (most significant bit in binary form), which is less than or equal to the given number N.\n1 2 3 4 5 6 7 8 9 long most_significant_bit(long n) { //changing all right side bits to 1. n = n | (n\u0026gt;\u0026gt;1); n = n | (n\u0026gt;\u0026gt;2); n = n | (n\u0026gt;\u0026gt;4); n = n | (n\u0026gt;\u0026gt;8); n = n | (n\u0026gt;\u0026gt;16); return (n+1)\u0026gt;\u0026gt;1; } Reverse Bits 1 2 3 4 5 6 7 8 9 uint32_t reverse_bits(uint32_t n) { unsigned int mask = 1\u0026lt;\u0026lt;31, res = 0; for(int i = 0; i \u0026lt; 32; ++i) { if(n \u0026amp; 1) res |= mask; mask \u0026gt;\u0026gt;= 1; n \u0026gt;\u0026gt;= 1; } return res; } 1 2 3 4 5 6 7 8 9 uint32_t reverse_bits(uint32_t n) { uint32_t mask = 1, ret = 0; for(int i = 0; i \u0026lt; 32; ++i){ ret \u0026lt;\u0026lt;= 1; if(mask \u0026amp; n) ret |= 1; mask \u0026lt;\u0026lt;= 1; } return ret; } 1 2 3 4 5 6 7 8 uint32_t reverse_bits(uint32_t n) { n = (n \u0026gt;\u0026gt; 16) | (n \u0026lt;\u0026lt; 16); n = ((n \u0026amp; 0xff00ff00) \u0026gt;\u0026gt; 8) | ((n \u0026amp; 0x00ff00ff) \u0026lt;\u0026lt; 8); n = ((n \u0026amp; 0xf0f0f0f0) \u0026gt;\u0026gt; 4) | ((n \u0026amp; 0x0f0f0f0f) \u0026lt;\u0026lt; 4); n = ((n \u0026amp; 0xcccccccc) \u0026gt;\u0026gt; 2) | ((n \u0026amp; 0x33333333) \u0026lt;\u0026lt; 2); n = ((n \u0026amp; 0xaaaaaaaa) \u0026gt;\u0026gt; 1) | ((n \u0026amp; 0x55555555) \u0026lt;\u0026lt; 1); return n; } 1 2 3 4 5 6 7 8 9 10 11 const uint32_t M1 = 0x55555555; // 01010101010101010101010101010101 const uint32_t M2 = 0x33333333; // 00110011001100110011001100110011 const uint32_t M4 = 0x0f0f0f0f; // 00001111000011110000111100001111 const uint32_t M8 = 0x00ff00ff; // 00000000111111110000000011111111 uint32_t reverse_bits(uint32_t n) { n = n \u0026gt;\u0026gt; 1 \u0026amp; M1 | (n \u0026amp; M1) \u0026lt;\u0026lt; 1; n = n \u0026gt;\u0026gt; 2 \u0026amp; M2 | (n \u0026amp; M2) \u0026lt;\u0026lt; 2; n = n \u0026gt;\u0026gt; 4 \u0026amp; M4 | (n \u0026amp; M4) \u0026lt;\u0026lt; 4; n = n \u0026gt;\u0026gt; 8 \u0026amp; M8 | (n \u0026amp; M8) \u0026lt;\u0026lt; 8; return n \u0026gt;\u0026gt; 16 | n \u0026lt;\u0026lt; 16; } Bitwise AND of Numbers Range Given a range [m, n] where 0 \u0026lt;= m \u0026lt;= n \u0026lt;= 2147483647, return the bitwise AND of all numbers in this range, inclusive.\n1 2 3 4 5 6 7 8 9 int range_bitwise_and(int m, int n) { int a = 0; while(m != n) { m \u0026gt;\u0026gt;= 1; n \u0026gt;\u0026gt;= 1; a++; } return m\u0026lt;\u0026lt;a; } Number of 1 Bits Write a function that takes an unsigned integer and returns the number of ’1\u0026rsquo; bits it has (also known as the Hamming weight).\n1 2 3 4 5 6 7 8 int hamming_weight(uint32_t n) { int count = 0; while(n) { n = n\u0026amp;(n-1); count++; } return count; } 1 2 3 4 5 6 7 8 9 int hamming_weight(uint32_t n) { long mask = 1; int count = 0; for(int i = 0; i \u0026lt; 32; ++i){ // 31 will not do, delicate if(mask \u0026amp; n) count++; mask \u0026lt;\u0026lt;= 1; } return count; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 int hamming_weight(uint32_t z) { // each bit in n is a one-bit integer that indicates // how many bits are set in that bit z = (z \u0026amp; 0x55555555) + ((z \u0026gt;\u0026gt; 1) \u0026amp; 0x55555555); // Now every two bits are a two bit integer that indicate // how many bits were set in those two bits in the original number z = (z \u0026amp; 0x33333333) + ((z \u0026gt;\u0026gt; 2) \u0026amp; 0x33333333); // Now we\u0026#39;re at 4 bits z = (z \u0026amp; 0x0f0f0f0f) + ((z \u0026gt;\u0026gt; 4) \u0026amp; 0x0f0f0f0f); z = (z \u0026amp; 0x00ff00ff) + ((z \u0026gt;\u0026gt; 8) \u0026amp; 0x00ff00ff); z = (z \u0026amp; 0x0000ffff) + ((z \u0026gt;\u0026gt; 16) \u0026amp; 0x0000ffff); // 32 bits return (int)z; } Exchange Two Integers without Additional Parameters 1 2 3 a = a ^ b; b = a ^ b; a = a ^ b; Find Two Numbers Appearing Twice 1 2 3 4 5 6 7 8 9 10 11 vector\u0026lt;int\u0026gt; single_number(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int sum=0; for(int\u0026amp; num:nums)sum^=num; int type1=sum==INT_MIN?sum:sum\u0026amp;(-sum), sum1=0; for(int\u0026amp; num:nums){ if(num\u0026amp;type1){ sum1^=num; } } return {sum1, sum^sum1}; } Binary Number with Alternating Bits 1 2 3 4 bool has_alternating_bits(int n) { long a = n ^ (n \u0026gt;\u0026gt; 1); return (a \u0026amp; (a + 1)) == 0; } Number Complement The complement of an integer is the integer you get when you flip all the 0\u0026rsquo;s to 1\u0026rsquo;s and all the 1\u0026rsquo;s to 0\u0026rsquo;s in its binary representation.\n1 2 3 4 5 6 7 8 9 int find_complement(int num) { int highestBit=0; for(int i=1;i\u0026lt;31;i++){ if(num\u0026gt;=(1\u0026lt;\u0026lt;i))highestBit=i; else break; } int mask=highestBit==30?0x7fffffff:((1\u0026lt;\u0026lt;(highestBit+1))-1); return num^mask; } Counting Bits Given an integer n, return an array ans of length n + 1 such that for each i (0 \u0026lt;= i \u0026lt;= n), ans[i] is the number of 1\u0026rsquo;s in the binary representation of i.\n1 2 3 4 5 6 7 vector\u0026lt;int\u0026gt; count_bits(int n) { vector\u0026lt;int\u0026gt;res(n+1); for(int i=1;i\u0026lt;=n;i++){ res[i]=res[i\u0026gt;\u0026gt;1]+(i\u0026amp;1); } return res; } Maximum Product of Word Length Given a string array words, return the maximum value of length(word[i]) * length(word[j]) where the two words do not share common letters. If no such two words exist, return 0.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 int max_product(vector\u0026lt;string\u0026gt;\u0026amp; words) { int size=words.size(); unordered_map\u0026lt;int, int\u0026gt;m; for(const string\u0026amp; word:words){ int temp=0; for(const char\u0026amp; c:word){ temp|=(1\u0026lt;\u0026lt;(c-\u0026#39;a\u0026#39;)); // use bit to save space } if(!m.count(temp)||word.size()\u0026gt;m[temp]){ m[temp]=word.size(); } } int res=0; for(const auto\u0026amp; mask1:m){ for(const auto\u0026amp; mask2:m){ if((mask1.first\u0026amp;mask2.first)==0){ res=max(res, mask1.second*mask2.second); } } } return res; } References A summary: how to use bit manipulation to solve problems easily and efficiently What USEFUL bitwise operator code tricks should a developer know about? Additional Resources Bit Twiddling Hacks ","date":"2022-05-28T00:00:00Z","permalink":"https://zychimne.github.io/posts/bit-manipulation/","title":"Bit Manipulation"},{"content":"Frameworks I keep asking myself what modern front-end development frameworks would be like. A lot of websites are bootstrapped from create-react-app, Next, Umi and other front-end frameworks today. They save the users a lot of trouble from setting up the configuration themselves. I haven\u0026rsquo;t used Next to build large-scale applications, so I would share my experience with create-react-app, Umi 3 and Umi 4. It is note worthy that Umi 4 is still in the release candidate phase (2022/05/27), so it is not as stable and reliable as Umi 3 and I wouldn\u0026rsquo;t recommend you to use it in enterprise projects.\nCreate React App Create React App is very friendly for people who are working with react for the first time. It creates a starter template and configures Webpack, Babel and other tools. Users do not need to know how to set them up. For advanced users, self-configured files will overwrite default settings, so it works perfectly for both starters and advanced developers. It has built-in CSS module support, and users can change css file extension to module to enable this feature.\nUmi Create React App is fine for small applications. However, when it comes to large-scale front-end application, create-react-app suffers from long webpack bundling time, no server side rendering support, difficult configurations and routings, etc. This is where Next and Umi is good at, especially when people first use them to support server side rendering. React itself is client side rendering, which means that when users request a web page from a server, the server returns an html with minimal elements and links pointing to react javascript. The browser needs to parse the javascript to render the whole page, which is a much slower process and has worse search engine optimization than server side rendering.\nUmi supports common features like CSS modules, routing, server side rendering, mock, plugins, webpack 5, lazy loading, etc. Developers do not have to know how to write code to make them work. Umi 3 is very stable as an enterprise level front-end framework. It locks the dependencies by default. Developers do not need to worry about waking up the next day and finding your dependencies are down.\nUpgrading to Umi 4 from Umi 3, except some api breaking changes and default to react 18, I can tell that bundling process is much faster than before. Umi 4 has MSFU (Module Federation Speed Up) V3. It compiles dependencies of the application once and for ever. When the components of the application change, it doesn\u0026rsquo;t have to compile the whole application from scratch. I didn\u0026rsquo;t turn MSFU on in Umi 3 because some libraries are not compatible with Webpack 5, and there are many warnings and errors showing up in the terminal. However, when it comes to Umi 4, everything has changed. MSFU is enabled by default, and the bundling process is very smooth. No more warnings and errors, and faster than you can ever imagine. Developing web pages with Umi 4 is a much more enjoyable experience than ever.\nHugo, and other static site generators As you can tell, my blog website is generated by Hugo. Hugo is fast, reliable, and easy to use. I can write blogs in markdown format, and Hugo will compile it to bundled html, css and javascript. I used to use Jekyll to set up my website. In the past, I forked a template from github page academic template and filled it with my own information. If I want to debug it locally, I need to install gem, a tool for ruby library management. What\u0026rsquo;s more, the website of Jekyll looks a bit out-dated, so I began to look for Jekyll alternatives. It seems like I have two choices, Hexo and Hugo. Hexo is written in javascript and relies on npm (node package manager) and node to run. Hugo is written in Go, a programming language known for speed and parallelization. I was not sure which one to use, as they both seem to be a decent choice for me. A large part of personal websites is themes, as we all want our website to look modern, beautiful and comfortable. Hugo\u0026rsquo;s themes caught my eye. I know this is what I want exactly when I first saw it. Beautiful layouts, smooth animations, and the experience of reading blogs is like reading a magazine.\nSetting up Hugo is easy. On macOS, run\n1 2 3 4 5 6 7 8 9 10 brew install hugo # install hugo on macOS hugo new site your-site-name # create a new folder with starter template git init # init git version control git submodule add your-path git@your-theme-git # add your chosen theme # Follow the instructions of setup in your chosen theme hugo server -D # start hugo server and you can see your page on localhost hugo --minify # build your site to release version # If you want to integrate with github pages, # don\u0026#39;t forget to set up github action workflows # and change page source to gh-pages. After running to above command, I can write blogs in /content/posts/my-blog-name/index.md and publish them when I push my code to github and they will get deployed automatically. Hugo is extremely fast, the github action usually takes less than 30 seconds to run.\nConclusion For developers who want to try or build a simple react website, I would recommend create-react-app. If the website is complicated, I believe Next and Umi would be a better choice. If you want to write blogs only, you can\u0026rsquo;t miss Hugo.\nAPIs Intersection Observer We can use Intersection Observer to find out whether an element is entering, inside or outside its container or viewport (MDN Docs of Intersection Observer). Chrome 51 (Release Date: June 13, 2016) first supports this API, and thus developers have more control over the elements. In the past, to find out whether an element is inside the viewport,\n1 2 3 4 5 6 7 8 9 10 11 12 13 const isElementInViewport = (element: HTMLElement) =\u0026gt; { const rect = element.getBoundingClientRect(); return ( rect.top \u0026gt;= 0 \u0026amp;\u0026amp; rect.left \u0026gt;= 0 \u0026amp;\u0026amp; rect.bottom \u0026lt;= (window.innerHeight || document.documentElement.clientHeight) /* or $(window).height() */ \u0026amp;\u0026amp; rect.right \u0026lt;= (window.innerWidth || document.documentElement.clientWidth) /* or $(window).width() */ ); }; But now, with intersection observer, we can use a callback. The callback will run when the intersection observer is initialized and when the element enter or leave the container. Therefore, to load data only when the element is inside the container, we need to use isIntersecting property to prevent undesirable behaviors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 const observer = new IntersectionObserver( (entries: IntersectionObserverEntry[]) =\u0026gt; { entries.forEach((entry) =\u0026gt; { if (entry.isIntersecting) { console.log(\u0026#39;Enter\u0026#39;); position(\u0026#39;VISIBLE\u0026#39;); // Do things if visible // For example, fetch data from the server return; } console.log(\u0026#39;Leave\u0026#39;); if (entry.boundingClientRect.top \u0026gt; 0) { position(\u0026#39;BELOW\u0026#39;); // Do things if below } else { position(\u0026#39;ABOVE\u0026#39;); // Do things if above } }); }, { root: null, threshold: 0, } ); More specifically, in React, if we want to observer multiple elements,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import React, { useRef, useEffect } from \u0026#39;react\u0026#39;; const Index = () =\u0026gt; { const refs = useRef\u0026lt;HTMLElement\u0026gt;(Array(n).fill(null)); useEffect(() =\u0026gt; { const callback = (entries: IntersectionObserverEntry[]) =\u0026gt; { // Do something, like entries.forEach((entry) =\u0026gt; { if (entry.isIntersecting) { fetch(); } }); }; const observer = new IntersectionObserver(callback, { root: null, threshold: 0, }); refs.forEach((element) =\u0026gt; { if (element) observer.observe(element); }); return () =\u0026gt; { observer.disconnect(); }; }, []); return ( \u0026lt;\u0026gt; {refs.map((item, index) =\u0026gt; { return \u0026lt;div ref={(el) =\u0026gt; (refs.current[index] = el)} key={index}\u0026gt;\u0026lt;/div\u0026gt;; })} \u0026lt;/\u0026gt; ); }; Promise Implement an A+ Promise in TypeScript MyPromise\nReferences juejin Promise A+ ","date":"2022-05-27T00:00:00Z","permalink":"https://zychimne.github.io/posts/modern-frontend-development/","title":"Modern Front-end Development"},{"content":"Responsive Web Design @media @media is not a new feature, since Chrome supports it in the first version. It is especially useful for displaying different layouts on devices with different screen sizes. As it uses hardware information from devices, it raises security concerns about identifying a device. Browser may choose to return default values rather than actual information when users activate no-tracking mode.\n1 2 3 4 5 @media screen and (min-width: 900px) { .element { padding: 1rem; } } flex Sometimes fixed width and height are not the optimal solution for different devices, because their screen widths and heights are different. Therefore, let the browser decide will be a better option. To do this, we can set up max width, min width, flex basis, flex grow and flex shrink. (Note that flex basis, flex grow and flex basis can be defined with flex as a short-hand css property)\n1 2 3 .element { flex: 1 0 0px; } Displaying Units Generally, there are two catagories of displaying units, absolute units and relative units. The most commonly used absolute unit is px in web design and development. In relative units, vw / vh, em and rem are more popular. em is based on the font size of its parent and rem is based on the root font size. vw / vh is based on the viewport size. Read More.\nThey are useful for different scenarios, but it is recommended to use only one or two of them in the entire project to remain consistent.\nMobile Device Adaption Consistent Experience on Different Devices Using viewport width and viewport height is better than px because iOS devices and high-resolution Android Devices tend to have higher px width and px height, but the design process of the blueprint is for viewport. We can use PostCSS Plugins, to set up a basic device and then CSS px properties will be automatically converted to the viewport units.\nDisplaying Color with Alpha Channel Properly Android doesn\u0026rsquo;t support 8-digit hex color, so it will simply omit the color property. CSS minimizers like esbuild minimizes the color to an 8-digit hex, and thus this is where they are in conflict. The solution I found is to set the minify syntax for CSS to false on esbuild.\n","date":"2022-05-10T00:00:00Z","permalink":"https://zychimne.github.io/posts/css-tricks/","title":"CSS Tricks"},{"content":"Microsoft Docs\n","date":"2022-05-01T00:00:00Z","permalink":"https://zychimne.github.io/posts/cpp-lambda/","title":"C++ Lambda"},{"content":"","date":"2022-05-01T00:00:00Z","permalink":"https://zychimne.github.io/posts/react-essentials/","title":"React Essentials"},{"content":"Regular Expression Read More on Microsoft Documentation GitHub Repository Learn Regex\n","date":"2022-05-01T00:00:00Z","permalink":"https://zychimne.github.io/posts/regular-expression/","title":"Regular Expression"},{"content":"TypeScript Airbnb JavaScript Style Guide Airbnb React/JSX Style Guide Airbnb CSS / Sass Style Guide\n","date":"2022-05-01T00:00:00Z","permalink":"https://zychimne.github.io/posts/typescript-essentials/","title":"TypeScript Essentials"},{"content":"","date":"2022-03-01T00:00:00Z","permalink":"https://zychimne.github.io/posts/golang-essentials/","title":"Golang Essentials"},{"content":"How to define the meaning of a word One-Hot Vector Say we have 3 word, \u0026ldquo;natural\u0026rdquo;, \u0026ldquo;language\u0026rdquo;, \u0026ldquo;processing\u0026rdquo;, we can define:\n\u0026ldquo;nature\u0026rdquo;=[1, 0, 0], \u0026ldquo;language\u0026rdquo;=[0, 1, 0], \u0026ldquo;processing\u0026rdquo;=[0, 0, 1] Cons: With the increase of the words, the dimension of the vectors grows rapidly. Any vector timing another vector equals 0, we cannot use cosine to measure their similarity. Word2Vec The basic theory is that we can know the meanning of a word from the words beside them.\nSkip-Gram Model Say we have a phrase \u0026ldquo;natural language processing\u0026rdquo;, take \u0026ldquo;language\u0026rdquo; as a central word then we have: P(\u0026ldquo;natural\u0026rdquo;) = P(\u0026ldquo;natural\u0026rdquo;|\u0026ldquo;language\u0026rdquo;) P(\u0026ldquo;processing\u0026rdquo;)=P(\u0026ldquo;processing\u0026rdquo;|\u0026ldquo;language\u0026rdquo;) Generally, we have\n$$ P(Context Word = o| Center Word = c) = \\frac{e^{(u_o^Tv_c)}}{\\sum\\nolimits_{w\\in{Vocab}}e^{(u_w^Tv_c)}} $$\nLoss function would be\n$$ J(\\theta) = -\\frac{1}{T}logL(\\theta) = -\\frac{1}{T}\\sum_{t=1}^T\\sum_{-m\\leq j\\leq m\u0026amp;j\\neq0}logP(w_{t+j}|w_t; \\theta) $$\nNote that $L(\\theta)$ is the maximum likelihood function.\nSingle Value Decomposition We use a word-to-document co-occurrence matrix. Say we have two sentences, and the window size = 1. I like deep learning. I like natural language processing.\nI like deep learning natural language processing I 0 2 0 0 0 0 0 like 2 0 1 0 1 0 0 deep 0 1 0 1 0 0 0 learning 0 0 1 0 0 0 0 natural 0 1 0 0 0 1 0 language 0 0 0 0 1 0 1 processing 0 0 0 0 0 1 0 Then we can compress the matrix and store the vectors. GloVe As its name suggests, GloVe uses global information to build vectors. $X_{ij}$ means how many times word j has shown up in the context of word i, then $X_i = \\sum_kX_{ik}$ means all the words showing up in the context of word i. The probility of word j showing up in the context of word i is denoted as\n$$ P_{ij} = P(j|i) = X_{ij} / X_i $$\nLoss Function:\n$$ w_i * w_j = logP(i|j) $$\n$$ J = \\sum_{i, j = 1}^Vf(X_{ij})(w_i^T\\tilde{w_j} + b_i + \\tilde{b_j} - logX_{ij})^2 $$\nNamed Entity Recognition We want to find the names in the text and classfy them, for example:\nOnly France [LOC] and Britain [LOC] back Fischler [PER] \u0026rsquo;s proposal.\nWindow Classification We can use the words beside the target word to classify it.\nSay the window size = 2 and the target word is Paris. We can have a window like:\nmuseum in Paris are amazing\nx1 x2 x3 x4 x5\n$X_{window} = [x_1, x_2, x_3, x_4, x_5]$ If we have mulitple class, we can use a softmax classifier.\nSay x denotes the input word vector and y is the target class among k classes, then we have\n$$ P(y|x) = \\frac{e^{(W_yx)}}{\\sum_{c=1}^ke^{(W_cx)}} $$\nThe loss function is a cross entropy function:\n$$ J(\\theta) = \\frac{1}{N}\\sum_{i = 1}^N-log(\\frac{e^{(W_yx)}}{\\sum_{c = 1}^ke^{(W_cx)}}) $$\nSyntactic Structure Analaysis Constituency Parsing Analyse the words as part of speech [POS], and then connect them as phrases and finally sentences.\nSay we have a phrase like\nthe cute cat at the coffee shop\nIts structure is [DET] [ADJ] [NOUN] [PREP] [DET] [NOUN], and Noun Phrase = { [DET] [ADJ] [NOUN] }, Preposition Phrase = { [PREP] [DET] [NOUN] }, and they together make a sentence.\nDependency Parsing Transition-based Dependency Parsing Transition-based dependency parsing is very similar to a state machine, for a given sentence $S = {w_0w_1\\cdots w_n}$, the state has three parts, $(\\sigma, \\beta, A)$.\n$\\sigma$ is a stack of words.\n$\\beta$ is a buffer of words.\nA is a collection of dependency arc, every edge is like $(w_i, r, w_j)$, and r denotes the dependency between words.\nAt the initial state, $\\sigma$ contains ROOT $w_0$ only, and $\\beta$ contains the rest of the sentence, while A is an empty set.\nThere are 3 transition between states:\nSHIFT: pop the first word out of the buffer and put it in the stack. LEFT-ARC: add $(w_j, r, w_i)$ to the collection of edges, and $w_j$ is on the top of the stack while $w_i$ is the second top one of the stack. RIGHT-ARC: add $(w_i, r, w_j)$ to the collection of edges, and $w_i$ is on the top of the stack while $w_j$ is the second top one of the stack. Language Model The model is about :\nGiven {$x_1, x_2, \\cdots, x_n$}, predict $x_{n+1}$.\n$$ P(x^{t + 1}, x^t, \\cdots, x^1) = \\sum_{t = 1}^TP(x^{t+1} | x^t, \\cdots, x^1) $$\nN-Gram Model The most classic one is the N-Gram Model. For example, the bigram of \u0026ldquo;natural language processing\u0026rdquo; is \u0026ldquo;natural language\u0026rdquo; and \u0026ldquo;language processing\u0026rdquo;.\nThe basic assumption of the model is that\nthe probability of n-gram is propotional to the probability of the word. $P(x^{t + 1})$ is only related to the (n - 1) word before it. $$ P(x^{t + 1} | x^t, \\cdots, x^1) \\approxeq \\frac{count(x^{t + 1}, x^t, \\cdots, x^{t - n + 2})}{count(x^t, \\cdots, x^{t - n + 2})} $$ Cons: Sparsity Problem The storage grows very fast as the n grows. Recurrent Neural Network In a RNN, different parameters share a same matrix, and they are not limited to a fixed size window.\nHidden Layer:\n$$ h_t = \\sigma(W^{(hh)}h_{t - 1} + W^{ht}x_{[t]}) $$\nOutput:\n$$ \\hat{y_t} = softmax(W^{(S)}h_t) $$\nThe loss function is cross entropy loss.\nWe can use perplexity to evalute our language model.\n$$ perplexity = e^{J(\\theta)} $$\nPros:\nIt can deal with input of any length. We can use information with a long history. The size of the model does not increase with input size. The parameters of weights are shared efficiently. Cons: RNN is comparatively slow because it is not parallel. Gradient vanishing makes long history information hard to capture. Gradient Vanishing and Gradient Explosion In $W^{(t - k)}$, if |W| is smaller than 1, as t - k incrrases, W is likely to become very small so it is very likely for us to loss information from the past. In the opposite, if |W| is greater than 1, as t - k incrrases, W is likely to become very large and cause explosion.\nGradient clipping If the gradient is greater than the preset threshold, we clip the gradient.\nLong Short Term Memory (LSTM) We introduce cell states to store the long-term information, and they can be written and erased by control gate. A general LSTM: Input gate\n$$ i_t = \\sigma(W^{(i)}x_t + U^{(i)}h_{t - 1}) $$\nForget gate\n$$ f_t = \\sigma(W^{(f)}x_t + U^{(f)}h_{t - 1}) $$\nOutput gate\n$$ o_t = \\sigma(W^{(o)}x_t + U^{(o)}h_{t - 1}) $$\nNew memory cell\n$$ \\tilde{c_t} = tanh(W^{(c)}x_t + U^{(c)}h_{t - 1}) $$\nFinal memory cell\n$$ c_t = f_t\\circ c_{t - 1} + i_t\\circ \\tilde{c_t} $$\nHidden state\n$$ h_t = o_t\\circ tanh(c_t) $$\nGated Recurrent Unit (GRU) GRU is very similar to LSTM, it combines forget gate and input gate into an update gate, and the cell state is combined into hidden state. Update gate\n$$ z_t = \\sigma(W^{(z)}x_t + U^{(z)}h_{t - 1}) $$\nReset gate\n$$ r_t = \\sigma(W^{(r)}x_t + U^{(r)}h_{t - 1}) $$\nNew memory cell\n$$ \\tilde{h_t} = tanh(r_t\\circ Uh_{t - 1} + Wx_t) $$\nHidden state\n$$ h_t = (1 - z_t)\\circ \\tilde{h_t} + z_t\\circ h{t - 1} $$\nNeural Machine Translation Neural machine translation is generally a sequence to sequence model. It uses RNN as an encoder and another RNN as a decoder.\nLoss function:\n$$ J(\\theta) = \\frac{1}{T}\\sum_{t = 1}^T-log\\tilde{y_{x_{t + 1}}^{(t)}} $$\nWe choose the word with highest probability as the next input of the decoder.\nBilingual Evaluation Study (BLEU) Presion score: $p_n = \\frac{# matched n-grams}{# n-grams in the candidate translation}$ Weight: $w = \\frac{1}{2^n}$ Penalty: $\\beta = e^{min(0, 1 - \\frac{len_{ref}}{len_{MT}})}$ BLEU:\n$$ BLEU = \\beta\\prod_{i = 1}^kp_n^{w_n} $$\nAttention We can use the attention method to focus on the specific context of the current word.\nProcess: Get the hidden states of the encoder $(h_1, h_2, h_T)$. Say the hidden state of the current decoder is $s_{t - 1}, we can use it for the relationship between input index j and current output index. $e_{ij} = a(s_{t - 1}, h_j)$, in the form of the vector is $\\vec{e_t} = (a(s_{t - 1}, h_1), \\cdots, a(s_{t - 1}, h_T))$, a denotes a relationship operation between s and h. Compute the attention by softmaxing $\\vec{e_t}$: $\\vec{a_t} = softmax(\\vec{e_t})$ Compute the context vector: $\\vec{c_t} = \\sum_{j =1}^Ta_{ij}h_j$ Finally, we have the next hidden state of the decoder in the form $s_t = f(s_{t - 1}, y_{t - 1}, c_t)$, and the ouput would be $p(y_t|y_1, \\cdots, y_{t - 1}, \\vec{x}) = g(y_{i - 1}, s_i, c_i)$. Question Answering System We use the Stanford Question Answering Dataset.\nStanford Attention Reader We get the features of the questions with a Bidirectional LSTM. For the answers, we can use a Bidirectional LSTM to get the features of every word, and $Attention = (\\vec{feature_{q_i}}, \\vec{feature_{w_i}})$, so that we can infer the start position and the end position of the answer.\nThe loss function is:\n$$ L = -\\sum logP^{start}(a_{start}) - \\sum logP^{end}(a_{end}) $$\nBirectional Attention Flow The attention model here is bidirectional, because we have a query-to-context layer and a context to query layer.\nSimilar matrix:\n$$ S_{ij} = w_{sim}^T[c_i; q_j, c_i \\circ q_j]\\in R $$\nNote that $c_i$, $q_i$ denote context vector and query vector respectively.\nFor the context to query attention,\nAttention score:\n$$ \\alpha^i = softmax(S_{i,:})\\in R^M \\forall i\\in{1,\\cdots, N} $$\nWeighted vector:\n$$ a_i = \\sum_{j = 1}^M\\alpha^i_jq_j \\in R^{2h} \\forall i\\in{1,\\cdots, N} $$\nFor the query to context attention,\n$$ m_i = max_jS_{ij } \\in R \\forall i\\in{1,\\cdots, N} $$\n$$ \\beta = softmax(m)\\in R^N $$\n$$ c\u0026rsquo; = \\sum_{i = 1}^N\\beta^ic_i \\in R^{2h} $$\nThe output of the Attention Flow Layer would be\n$$ b_i = [c_i; a_i; c_i\\circ a_i; c_i\\circ c\u0026rsquo;] \\in R^{8h} \\forall i \\in{1, \\cdots, N} $$\nConvolutional Neural Network If the embedding of a word is a k-dimensional vector, a sentence is a matrix. With n word, the matrix size is n * k. We usually need to pad the sentence to have a deeper convolutional network.\nThe initiative of pooling is the stablize the network, when the input changes little.\nQuasi Recurrent Neural Network Quaso Recurrent Neural Network is a combination of convolutional neural network and recurrent neural network. The main idea is to use a convolutional neural network to extract features and replace the pooling layer with dynamic average pooling.\nFor the feature extraction, the candidate vector, forget gate, output gate is:\n$$ Z = tanh(W_z * X) $$\n$$ F = \\sigma(W_f * X) $$\n$$ O = \\sigma(W_o * X) $$\nThe dynamic average pooling is:\n$$ x_t = f_t\\cdot c_{t - 1} + (1 - f_t)\\cdot z_t $$\n$$ h_t = o_t\\cdot c_t $$\nSubword Model Character-level model We can use more layers of convolution, pooling and highway to solve the complexity problem.\nByte Pair Encoding Byte pair encodes n-grams of the highest frequency until its vocabulary reaches the preset threshold.\nFasttText It takes a word as bag of character n-gram.\nContextual Word Representation Embeddings from Language Models (ELMO) ELMO uses a bidirectional LSTM to compute the contextual embedding. The lower layer represents the basic grammar information, while the higher layer represents context information.\nForward language model:\n$$ p(t_1, t_2, \\cdots, t_N) = \\prod_{k = 1}^Np(t_k|t_1, t_2, \\cdots, t_{k - 1}) $$\nBackword language model:\n$$ p(t_1, t_2, \\cdots, t_N) = \\prod_{k = 1}^Np(t_k|t_{k + 1}, t_{k + 2}, \\cdots, t_N) $$\nThe initiative of the bidirectional LSTM is to maximaize:\n$$ \\sum_{k + 1}^N(logp(t_k|t_1, t_2, \\cdots, t{k - 1}) + logp(t_k|t_{k + 1}, t_{k + 2}, \\cdots, t_N)) $$\nFor the word at index k, it is represented by (2L + 1) vectors. One of them is an embedding not related to the context, while L layers of forward LSTM produces $\\vec{h*{kj}} related to the former paragraph and L layers of backward LSTM produces $\\vec{h*{kj}} for the rest of the paragraph. For the following layers, we have:\n$$ ELMO_k^{task} = E(R_k; \\theta^{task}) = \\gamma^{task}\\sum_{j = 0}^Ls_j^{task}h_{k, j}^{LM} $$\nNote that $s^{task}$ is softmaxed and $\\gamma^{task}$ is a scale parameter.\nGenerative Pre Training (GPT) The basic structure of the GPT is a transformer, which is used for unsupervised learning for the text, and then we can use the embedding for supervised fine-tuning for the following tasks.\nNote that it is one direction.\nBidirectional Encoder Representations from Transformers Bert is especially tuned for:\nPredict a word with k% missing. Predict a following sentence. Transformer Self Attention We compute the ralationship between words in self attention. The encoder has multiple self-attention layers, after which the input gets the context information for every word. The decoder has a similar structure, but it takes the output of itself and the output of encoder as its input.\nThe Structure of Self Attention MatMul Scale Mask SoftMax MatMul For the encoder, the input is Query, Key and Value, and $d_k$ denotes that dimension of query and vectors. $$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$ The Structure of a transformer Positional encoding Multi-Head attention Add \u0026amp; Normalize Feed forward Position encoding allows the model to learn about the position of a word in the sentence.\nMulti-head attention contains multiple self-attention layers, and different head may learn different information of the features.\nThe Add part is very similar to Residual Connection, which can pass the information of the former layer to the next one. ","date":"2020-09-01T00:00:00Z","permalink":"https://zychimne.github.io/posts/natural-language-processing-basics/","title":"Natural Language Processing Basics"}]